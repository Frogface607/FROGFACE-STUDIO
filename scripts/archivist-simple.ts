#!/usr/bin/env tsx
/**
 * –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∞—Ä—Ö–∏–≤–∞—Ä–∏—É—Å–∞ –±–µ–∑ workspace –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
 * –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª
 */

import * as fs from 'fs/promises'
import * as path from 'path'

const KNOWLEDGE_FILE = path.join(__dirname, '../knowledge-store.json')

// –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–Ω–∞–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞
let knowledgeStore: Map<string, any> = new Map()

async function loadKnowledge() {
  try {
    const data = await fs.readFile(KNOWLEDGE_FILE, 'utf-8')
    const items = JSON.parse(data)
    knowledgeStore = new Map(Object.entries(items))
  } catch {
    // –§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–∞—á–∏–Ω–∞–µ–º —Å –ø—É—Å—Ç–æ–π –±–∞–∑—ã
    knowledgeStore = new Map()
  }
}

async function saveKnowledge() {
  const items = Object.fromEntries(knowledgeStore)
  await fs.writeFile(KNOWLEDGE_FILE, JSON.stringify(items, null, 2), 'utf-8')
}

// –ó–∞–≥—Ä—É–∂–∞–µ–º env
const apiKey = process.env.OPENROUTER_API_KEY || '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'

async function callLLM(prompt: string, temperature = 0.7): Promise<string> {
  const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${apiKey}`,
      'HTTP-Referer': 'https://frogface.studio',
      'X-Title': 'Frogface Studio',
    },
    body: JSON.stringify({
      model: 'openai/gpt-4-turbo-preview',
      messages: [{ role: 'user', content: prompt }],
      temperature,
    }),
  })

  if (!response.ok) {
    throw new Error(`OpenRouter API error: ${response.status}`)
  }

  const data = await response.json()
  return data.choices[0]?.message?.content || ''
}

async function classifyContent(content: string): Promise<{ namespace: string; category: string }> {
  const prompt = `–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –∫ –∫–∞–∫–æ–º—É namespace –æ—Ç–Ω–æ—Å–∏—Ç—Å—è:

–î–æ—Å—Ç—É–ø–Ω—ã–µ namespace: copywriter, researcher, smm, psychologist, archivist, global

–ö–æ–Ω—Ç–µ–Ω—Ç:
"""
${content.substring(0, 2000)}
"""

–í–µ—Ä–Ω–∏ JSON:
{
  "namespace": "copywriter|researcher|smm|psychologist|archivist|global",
  "category": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"
}`

  try {
    const response = await callLLM(prompt, 0.3)
    const jsonMatch = response.match(/\{[\s\S]*\}/)
    if (jsonMatch) {
      return JSON.parse(jsonMatch[0])
    }
  } catch (error) {
    console.warn('–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:', error)
  }

  // Fallback
  const lower = content.toLowerCase()
  if (lower.includes('–ø–æ—Å—Ç') || lower.includes('–∞–Ω–æ–Ω—Å') || lower.includes('–∫–æ–Ω—Ü–µ—Ä—Ç')) {
    return { namespace: 'copywriter', category: 'post' }
  }
  if (lower.includes('–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω') || lower.includes('–∞–Ω–∞–ª–∏–∑')) {
    return { namespace: 'researcher', category: 'research' }
  }
  if (lower.includes('—Å—Ç–∏–ª—å') || lower.includes('smm') || lower.includes('—Å–æ—Ü—Å–µ—Ç')) {
    return { namespace: 'copywriter', category: 'style' }
  }
  return { namespace: 'global', category: 'general' }
}

function extractTextFromHTML(html: string): string {
  let text = html.replace(/<script[^>]*>[\s\S]*?<\/script>/gi, '')
  text = text.replace(/<style[^>]*>[\s\S]*?<\/style>/gi, '')
  text = text.replace(/<[^>]+>/g, ' ')
  text = text.replace(/&nbsp;/g, ' ').replace(/&amp;/g, '&').replace(/&lt;/g, '<').replace(/&gt;/g, '>').replace(/&quot;/g, '"').replace(/&#39;/g, "'")
  text = text.replace(/\s+/g, ' ').trim()
  return text
}

async function processFile(fileName: string, buffer: Buffer): Promise<string> {
  const ext = fileName.split('.').pop()?.toLowerCase() || ''
  
  if (ext === 'html' || ext === 'htm') {
    const htmlText = buffer.toString('utf-8')
    const titleMatch = htmlText.match(/<title[^>]*>([\s\S]*?)<\/title>/i)
    const title = titleMatch ? titleMatch[1].trim() : ''
    
    const headings: string[] = []
    const headingRegex = /<h[1-6][^>]*>([\s\S]*?)<\/h[1-6]>/gi
    let match
    while ((match = headingRegex.exec(htmlText)) !== null) {
      const heading = match[1].replace(/<[^>]+>/g, '').trim()
      if (heading) headings.push(heading)
    }
    
    const cleanText = extractTextFromHTML(htmlText)
    
    let result = ''
    if (title) result += `# ${title}\n\n`
    if (headings.length > 0) {
      result += '## –ó–∞–≥–æ–ª–æ–≤–∫–∏:\n'
      headings.forEach(h => result += `- ${h}\n`)
      result += '\n'
    }
    result += cleanText.substring(0, 10000)
    
    return result
  }
  
  if (ext === 'json') {
    try {
      const json = JSON.parse(buffer.toString('utf-8'))
      return JSON.stringify(json, null, 2)
    } catch {
      return buffer.toString('utf-8')
    }
  }
  
  return buffer.toString('utf-8')
}

async function addToKnowledge(content: string, namespace: string, metadata: any = {}): Promise<string> {
  const id = `${namespace}-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
  knowledgeStore.set(id, {
    id,
    content,
    namespace,
    metadata,
    createdAt: new Date().toISOString(),
  })
  await saveKnowledge()
  console.log(`‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ ${namespace}: ${id}`)
  return id
}

async function addText(text: string) {
  console.log('üìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—Å—Ç...')
  const classification = await classifyContent(text)
  
  console.log(`‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: ${classification.namespace} - ${classification.category}`)
  
  const id = await addToKnowledge(text, classification.namespace, {
    category: classification.category,
    archivedAt: new Date().toISOString(),
    source: 'cli',
  })
  
  console.log(`‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ! ID: ${id}`)
  console.log(`üìÅ Namespace: ${classification.namespace}`)
}

async function addFile(filePath: string) {
  console.log(`üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: ${filePath}`)
  
  const buffer = await fs.readFile(filePath)
  const fileName = path.basename(filePath)
  const content = await processFile(fileName, buffer)
  
  console.log(`üìù –ò–∑–≤–ª–µ—á–µ–Ω–æ ${content.length} —Å–∏–º–≤–æ–ª–æ–≤`)
  
  await addText(content)
}

async function stats() {
  const namespaces: Record<string, number> = {}
  
  // –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º Supabase, –ø–æ–ª—É—á–∞–µ–º —á–µ—Ä–µ–∑ –Ω–µ–≥–æ
  if (process.env.SUPABASE_URL && process.env.SUPABASE_SERVICE_KEY) {
    try {
      const { SupabaseKnowledgeAdapter } = require('../packages/core/knowledge-base/src/supabase-adapter')
      const adapter = new SupabaseKnowledgeAdapter({
        url: process.env.SUPABASE_URL,
        serviceKey: process.env.SUPABASE_SERVICE_KEY,
      })
      
      // –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ namespace (–Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å metadata)
      const allNamespaces = ['copywriter', 'researcher', 'smm', 'psychologist', 'archivist', 'global', 'test']
      for (const ns of allNamespaces) {
        const items = await adapter.getAll(ns)
        if (items.length > 0) {
          namespaces[ns] = items.length
        }
      }
    } catch (error) {
      console.warn('–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ Supabase:', error)
    }
  }
  
  // Fallback –Ω–∞ in-memory
  if (Object.keys(namespaces).length === 0) {
    for (const item of knowledgeStore.values()) {
      namespaces[item.namespace] = (namespaces[item.namespace] || 0) + 1
    }
  }
  
  console.log('üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n')
  if (Object.keys(namespaces).length === 0) {
    console.log('–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞')
  } else {
    for (const [ns, count] of Object.entries(namespaces)) {
      console.log(`${ns}: ${count} –∑–∞–ø–∏—Å–µ–π`)
    }
  }
}

async function list(ns?: string) {
  const namespace = ns || 'global'
  let items: any[] = []
  
  // –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º Supabase
  if (process.env.SUPABASE_URL && process.env.SUPABASE_SERVICE_KEY) {
    try {
      const { SupabaseKnowledgeAdapter } = require('../packages/core/knowledge-base/src/supabase-adapter')
      const adapter = new SupabaseKnowledgeAdapter({
        url: process.env.SUPABASE_URL,
        serviceKey: process.env.SUPABASE_SERVICE_KEY,
      })
      const supabaseItems = await adapter.getAll(namespace)
      items = supabaseItems.map((item: any) => ({
        id: item.id,
        content: item.content,
        metadata: item.metadata || {},
      }))
    } catch (error) {
      console.warn('–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–∑ Supabase:', error)
    }
  }
  
  // Fallback –Ω–∞ in-memory
  if (items.length === 0) {
    for (const item of knowledgeStore.values()) {
      if (!namespace || item.namespace === namespace) {
        items.push(item)
      }
    }
  }
  
  console.log(`\nüìã –ó–∞–ø–∏—Å–∏ –≤ ${namespace}: ${items.length}\n`)
  items.forEach((item, idx) => {
    console.log(`${idx + 1}. [${item.metadata?.category || 'uncategorized'}]`)
    console.log(`   ${item.content.substring(0, 100)}...`)
    console.log(`   ID: ${item.id}\n`)
  })
}

// Main
async function main() {
  // –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–Ω–∞–Ω–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
  await loadKnowledge()
  
  const args = process.argv.slice(2)
  const command = args[0]

  switch (command) {
    case 'add':
      if (!args[1]) {
        console.error('‚ùå –£–∫–∞–∂–∏ —Ç–µ–∫—Å—Ç')
        process.exit(1)
      }
      await addText(args[1])
      break
      
    case 'add-file':
      if (!args[1]) {
        console.error('‚ùå –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É')
        process.exit(1)
      }
      await addFile(args[1])
      break
      
    case 'stats':
      await stats()
      break
      
    case 'list':
      await list(args[1])
      break
      
    default:
      console.log(`
üì¶ –ê—Ä—Ö–∏–≤–∞—Ä–∏—É—Å CLI (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)

–ö–æ–º–∞–Ω–¥—ã:
  add "—Ç–µ–∫—Å—Ç"            - –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç
  add-file file.html     - –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∞–π–ª
  stats                  - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
  list [namespace]       - —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π

–ü—Ä–∏–º–µ—Ä—ã:
  pnpm tsx scripts/archivist-simple.ts add "–ú–æ–π —Ç–µ–∫—Å—Ç"
  pnpm tsx scripts/archivist-simple.ts add-file "—Ñ–∞–π–ª.html"
  pnpm tsx scripts/archivist-simple.ts stats
  pnpm tsx scripts/archivist-simple.ts list copywriter
`)
  }
}

main().catch(console.error)
